#! /usr/bin/env python
# -*- coding: utf-8 -*-

import sys
from io import open
from argparse import ArgumentParser, FileType

from six import text_type as unicode

from base import Sequence, TextFile
from detect import Detector
from tokenize import SentenceTokenizer, WordTokenizer


def detect(args):
  for l in args.input:
    print(Detector().detect(l))


def cat(args):
  """ Concatenate the content of the input file."""
  for l in args.input:
    print(l.strip())


def segment(args):
  if args.lang != 'detect':
    lang  = args.lang
  else:
    raise NotImplementedError("TODO: detect language")

  w_tokenizer = WordTokenizer(locale=lang)
  s_tokenizer = SentenceTokenizer(locale=lang)

  if args.only_sent:
    for l in args.input:
      seq = Sequence(l)
      if not seq.empty(): print(s_tokenizer.transform(seq))

  elif args.only_word:
    for l in args.input:
      seq = Sequence(l)
      if not seq.empty(): print(w_tokenizer.transform(seq))

  else:
    for l in args.input:
      seq = Sequence(l)
      sents = s_tokenizer.transform(seq)
      words = w_tokenizer.transform(seq)
      for tokenized_sent in words.split(sents):
        if not tokenized_sent.empty():
          print(u' '.join(tokenized_sent.tokens()))


if __name__ == "__main__":
  parser = ArgumentParser("polyglot")
  subparsers = parser.add_subparsers(title='tools',
                                     description='multilingual tools for all languages',
                                     help='try them')
  parser.add_argument('--lang', default='detect', help='Language to be processed')
  parser.add_argument('--format', default='text', help='File format')
  parser.add_argument('--delimiter', default=u'\n', help='Delimiter that '
                      'seperates documents, records or even sentences.')
  parser.add_argument('--workers', default=1, type=int,
                      help='Number of parallel processes.')
  parser.add_argument('--input', nargs='?', type=TextFile,
                        default=sys.stdin)
  #parser.set_defaults(func=cat)

  # Language detector
  detector = subparsers.add_parser('detect')
  detector.add_argument('--fine', action='store_true', default=False,
                        dest='fine_grain')
  detector.set_defaults(func=detect)

  # Morphological Analyzer
  morph = subparsers.add_parser('morph')
  morph.set_defaults(func=morph)

  # Tokenizer
  tokenizer = subparsers.add_parser('tokenize')
  tokenizer.add_argument("--only-sent", default=False, action="store_true")
  tokenizer.add_argument("--only-word", default=False, action="store_true")
  tokenizer.set_defaults(func=segment)

  # Named Entity Chunker
  subparsers.add_parser('ner')

  # Sentiment Analysis
  subparsers.add_parser('sentiment')

  args = parser.parse_args()

  args.delimiter = unicode(args.delimiter.decode('unicode-escape'))
  args.input.delimiter = args.delimiter
  args.func(args)
