#! /usr/bin/env python
# -*- coding: utf-8 -*-

import sys
from io import open
from argparse import ArgumentParser, FileType
from collections import Counter
from concurrent.futures import ProcessPoolExecutor
import logging

from six import text_type as unicode
from six import iteritems

from icu import Locale

from base import Sequence, TextFile
from mapping import CountedVocabulary
from detect import Detector
from tokenize import SentenceTokenizer, WordTokenizer

logger = logging.getLogger(__name__)
LOGFORMAT = "%(asctime).19s %(levelname)s %(filename)s: %(lineno)s %(message)s"

def vocab_counter(args):
  """Calculate the vocabulary."""
  v = CountedVocabulary.from_textfile(textfile=args.input)
  if args.min_count > 1: 
    v = v.min_count(args.min_count)
  if args.most_freq > 0:
    v = v.most_frequent(args.most_freq)
  print(v)


def detect(args):
  for l in args.input:
    if l.strip():
      print(Detector(l).name)


def cat(args):
  """ Concatenate the content of the input file."""
  for l in args.input:
    print(l.strip())


def segment(args):
  if args.lang != 'detect':
    lang  = args.lang
  else:
    raise NotImplementedError("TODO: detect language")

  w_tokenizer = WordTokenizer(locale=lang)
  s_tokenizer = SentenceTokenizer(locale=lang)

  if args.only_sent:
    for l in args.input:
      seq = Sequence(l)
      if not seq.empty(): print(s_tokenizer.transform(seq))

  elif args.only_word:
    for l in args.input:
      seq = Sequence(l)
      if not seq.empty(): print(w_tokenizer.transform(seq))

  else:
    for l in args.input:
      seq = Sequence(l)
      sents = s_tokenizer.transform(seq)
      words = w_tokenizer.transform(seq)
      for tokenized_sent in words.split(sents):
        if not tokenized_sent.empty():
          print(u' '.join(tokenized_sent.tokens()))


if __name__ == "__main__":
  parser = ArgumentParser("polyglot",
                          conflict_handler='resolve')
  subparsers = parser.add_subparsers(title='tools',
                                     description='multilingual tools for all languages')
  parser.add_argument('--lang', default='detect', help='Language to be processed')
  parser.add_argument('--format', default='text', help='File format')
  parser.add_argument('--delimiter', default=u'\n', help='Delimiter that '
                      'seperates documents, records or even sentences.')
  parser.add_argument('--workers', default=1, type=int,
                      help='Number of parallel processes.')
  parser.add_argument('--input', nargs='?', type=TextFile,
                      default=TextFile(sys.stdin.fileno()))
  parser.add_argument("-l", "--log", dest="log", help="log verbosity level",
                      default="INFO")


  # Language detector
  detector = subparsers.add_parser('detect',
                                   help="Detect languages")
  detector.add_argument('--fine', action='store_true', default=False,
                        dest='fine_grain')
  detector.set_defaults(func=detect)

  # Morphological Analyzer
  morph = subparsers.add_parser('morph')
  morph.set_defaults(func=morph)

  # Tokenizer
  tokenizer = subparsers.add_parser('tokenize')
  group1= tokenizer.add_mutually_exclusive_group()
  group1.add_argument("--only-sent", default=False, action="store_true",
                      help="Segment sentences without word tokenization")
  group1.add_argument("--only-word", default=False, action="store_true",
                      help="Tokenize words without sentence segmentation")
  tokenizer.set_defaults(func=segment)

  # Vocabulary Counter
  counter = subparsers.add_parser('count')
  group1= counter.add_mutually_exclusive_group()
  group1.add_argument("--min-count", type=int, default=1,
                      help="Ignore all words that appear <= min_freq.")
  group1.add_argument("--most-freq", type=int, default=-1,
                      help="Consider only the most frequent k words.")
  counter.set_defaults(func=vocab_counter)

  # Concatenate the input file
  catter = subparsers.add_parser('cat')
  catter.set_defaults(func=cat)

  # Named Entity Chunker
  subparsers.add_parser('ner')

  # Sentiment Analysis
  subparsers.add_parser('sentiment')

  args = parser.parse_args()
  numeric_level = getattr(logging, args.log.upper(), None)
  logging.basicConfig(format=LOGFORMAT)
  logger.setLevel(numeric_level)

  #parser.set_defaults(func=cat)

  if args.lang == 'detect':
    header = 4096
    text = args.input.peek(header)
    lang = Detector(text)
    args.lang = lang.code
    logger.info("Language {} is detected while reading the first {} bytes"
                ".".format(lang.name, lang.read_bytes))

  args.delimiter = unicode(args.delimiter.decode('unicode-escape'))
  args.input.delimiter = args.delimiter
  args.func(args)
