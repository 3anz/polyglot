#! /usr/bin/env python
# -*- coding: utf-8 -*-

import sys
from io import open
from argparse import ArgumentParser, FileType

from six import text_type as unicode

from base import Sequence, TextFile
from detect import Detector
from tokenize import SentenceTokenizer, WordTokenizer


def detect(args):
  for l in args.input:
    print(Detector().detect(l))


def cat(args):
  """ Concatenate the content of the input file."""
  for l in args.input:
    print(l.strip())


def segment(args):
  if args.lang != 'detect':
    lang  = args.lang
  else:
    raise NotImplementedError("TODO: detect language")

  w_tokenizer = WordTokenizer(locale=lang)
  s_tokenizer = SentenceTokenizer(locale=lang)

  if args.only_sent:
    for l in args.input:
      seq = Sequence(l)
      if not seq.empty(): print(s_tokenizer.transform(seq))

  elif args.only_word:
    for l in args.input:
      seq = Sequence(l)
      if not seq.empty(): print(w_tokenizer.transform(seq))

  else:
    for l in args.input:
      seq = Sequence(l)
      sents = s_tokenizer.transform(seq)
      words = w_tokenizer.transform(seq)
      for tokenized_sent in words.split(sents):
        if not tokenized_sent.empty():
          print(u' '.join(tokenized_sent.tokens()))


if __name__ == "__main__":
  parser = ArgumentParser("polyglot",
                          conflict_handler='resolve')
  subparsers = parser.add_subparsers(title='tools',
                                     description='multilingual tools for all languages')
  parser.add_argument('--lang', default='detect', help='Language to be processed')
  parser.add_argument('--format', default='text', help='File format')
  parser.add_argument('--delimiter', default=u'\n', help='Delimiter that '
                      'seperates documents, records or even sentences.')
  parser.add_argument('--workers', default=1, type=int,
                      help='Number of parallel processes.')
  parser.add_argument('--input', nargs='?', type=TextFile,
                      default=TextFile(sys.stdin.fileno()))
  #parser.set_defaults(func=cat)

  # Language detector
  detector = subparsers.add_parser('detect',
                                   help="Detect languages")
  detector.add_argument('--fine', action='store_true', default=False,
                        dest='fine_grain')
  detector.set_defaults(func=detect)

  # Morphological Analyzer
  morph = subparsers.add_parser('morph')
  morph.set_defaults(func=morph)

  # Tokenizer
  tokenizer = subparsers.add_parser('tokenize')
  group1= tokenizer.add_mutually_exclusive_group()
  group1.add_argument("--only-sent", default=False, action="store_true",
                      help="Segment sentences without word tokenization")
  group1.add_argument("--only-word", default=False, action="store_true",
                      help="Tokenize words without sentence segmentation")
  tokenizer.set_defaults(func=segment)

  # Package downloader
  downloader = subparsers.add_parser('download')
  downloader.add_argument("--dir", dest="dir",
    help="download package to directory DIR", metavar="DIR")
  downloader.add_argument("--quiet", dest="quiet", action="store_true",
    default=False, help="work quietly")
  downloader.add_argument("--force", dest="force", action="store_true",
    default=False, help="download even if already installed")
  downloader.add_argument("--exit-on-error", dest="halt_on_error", action="store_true",
    default=False, help="exit if an error occurs")
  downloader.add_argument("--url", dest="server_index_url",
    default=None, help="download server index url")
  downloader.set_defaults(func=download)


  # Named Entity Chunker
  subparsers.add_parser('ner')

  # Sentiment Analysis
  subparsers.add_parser('sentiment')

  args = parser.parse_args()

  args.delimiter = unicode(args.delimiter.decode('unicode-escape'))
  args.input.delimiter = args.delimiter
  args.func(args)
