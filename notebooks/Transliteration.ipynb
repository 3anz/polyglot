{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_dir = '/media/data/code/polyglot/'\n",
    "if exp_dir not in sys.path:\n",
    "  sys.path.insert(0, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.load import load_transliteration_table, locate_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.transliteration import Transliterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models were trained on a combination of:\n",
    "- Original CONLL datasets after the tags were converted using the [universal POS tables](http://universaldependencies.github.io/docs/tagset-conversion/index.html).\n",
    "- Universal Dependencies 1.0 corpora whenever they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.downloader import downloader\n",
    "print(\", \".join(downloader.supported_languages(\"pos2\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Necessary Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package embeddings2.en to\n",
      "[polyglot_data]     /home/rmyeid/polyglot_data...\n",
      "[polyglot_data]   Package embeddings2.en is already up-to-date!\n",
      "[polyglot_data] Downloading package pos2.en to\n",
      "[polyglot_data]     /home/rmyeid/polyglot_data...\n",
      "[polyglot_data]   Package pos2.en is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "polyglot download embeddings2.en pos2.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tag each word in the text with one part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = \"\"\"We will meet at eight o'clock on Thursday morning.\"\"\"\n",
    "text = Text(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query all the tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وي\n",
      "ويل\n",
      "ميت\n",
      "ات\n",
      "ييايت\n",
      "أوكلوك\n",
      "ون\n",
      "ثورسداي\n",
      "مورنينغ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in text.transliterate(\"ar\"):\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Notice, if we do not pass `--lang` the language code, the detector will bem used to detect the language of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia posted a World Cup record total of 417 - 6 as they beat Afghanistan by 275 runs .\n",
      "David Warner hit 178 off 133 balls , Steve Smith scored 95 while Glenn Maxwell struck 88 in 39 deliveries in the Pool A encounter in Perth .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015-03-11 22:20:03 INFO __main__.py: 275 Language English is detected while reading the first 1128 bytes.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tok_file=/tmp/cricket.tok.txt\n",
    "polyglot tokenize --input testdata/cricket.txt > $tok_file\n",
    "head -n 2 $tok_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia       اوستراليا       \n",
      "posted          بوستيد          \n",
      "a               ا               \n",
      "World           وورلد           \n",
      "Cup             كوب             \n",
      "record          ريكورد          \n",
      "total           توتال           \n",
      "of              وف              \n",
      "417                             \n",
      "-                               \n",
      "6                               \n",
      "as              اس              \n",
      "they            ثي              \n",
      "beat            بيت             \n",
      "Afghanistan     افغانيستان      \n",
      "by              بي              \n",
      "275                             \n",
      "runs            رونس            \n",
      ".                               \n",
      "\n",
      "David           دافيد           \n",
      "Warner          وارنر           \n",
      "hit             هيت             \n",
      "178                             \n",
      "off             وفف             \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tok_file=/tmp/cricket.tok.txt\n",
    "polyglot --lang en transliteration --target ar --input $tok_file | head -n 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nesting steps\n",
    "We can nest the tokenization and POS tagging in a simple bash pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which           ويكه            \r\n",
      "India           ينديا           \r\n",
      "beat            بيت             \r\n",
      "Bermuda         بيرمودا         \r\n",
      "in              ين              \r\n",
      "Port            بورت            \r\n",
      "of              وف              \r\n",
      "Spain           سباين           \r\n",
      "in              ين              \r\n",
      "2007                            \r\n",
      ",                               \r\n",
      "which           ويكه            \r\n",
      "was             واس             \r\n",
      "equalled        يكالليد         \r\n",
      "five            فيفي            \r\n",
      "days            دايس            \r\n",
      "ago             اغو             \r\n",
      "by              بي              \r\n",
      "South           سووث            \r\n",
      "Africa          افريكا          \r\n",
      "in              ين              \r\n",
      "their           ثير             \r\n",
      "victory         فيكتوري         \r\n",
      "over            وفير            \r\n",
      "West            ويست            \r\n",
      "Indies          يندييس          \r\n",
      "in              ين              \r\n",
      "Sydney          سيدني           \r\n",
      ".                               \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!polyglot --lang en tokenize --input testdata/cricket.txt |  polyglot --lang en transliteration --target ar | tail -n 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "\n",
    "This work is a direct implementation of the research being described in the [Polyglot: Distributed Word Representations for Multilingual NLP](http://www.aclweb.org/anthology/W13-3520) paper.\n",
    "The author of this library strongly encourage you to cite the following paper if you are using this software."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. code-block::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Universal Part of Speech Tagging](http://universaldependencies.github.io/docs/u/pos/index.html)\n",
    "- [Universal Dependencies 1.0](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1464)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
